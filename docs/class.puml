@startuml

class HyperTrainer {
  ハイパーパラメータ を決定するための実験を行う
  ..
  pre [None, Normalize, Singular]
  mid_depth [1,2,3,4,5,6]
  mid_width [5,10,20,40,60,80]
  afunc [Sigmoid, ReLu, Tanh]
  grad [Static1, Static2, Static3, Moment, AdGrad, Adam]

}

class NetTrainer {
  与えられた Net の学習を行う(w,bの調整をする)
  ..
  net
  best_net = None
  min_err = 10e38
  train(loop, x, d) : err[]
}

abstract class Net {
w [重み]
b [バイアス]
f [活性化関数]
g 重み管理
d 重み減衰
u [forward時の各層の出力の記録]
z [forward時の各層の出力の記録]
drop_rate = 0.0
w_mask [ドロップアウト用マスク]
b_mask [ドロップアウト用マスク]
add_pre_layer(layer_factory, activate_function, x, d)
add_layer([units], layer_factory, activate_function)
add_post_layer(units, layer_factory, activate_function)
set_weight_decay(weight_decay)
drop_out(rate)
drop_in()
forward(x) : y
backward(d,y)
}

interface LayerFactory {
Layer (w,b,f) の初期化を行う
..
create()
}

interface ActivateFunction{
活性化関数
..
calc(x)
differential(x)
delta(d,y)
}

interface Grad {
重み・バイアスの調整
..
dw [重み学習率]
db [バイアス学習率]
init()
eta(dWdE, dWdB)
}

interface WeightDecay {
重み減衰
..
rw [重みの正則化ペナルティ]
rb [バイアスの正則化ペナルティ]
init()
update(w,b)
}

HyperTrainer --> NetTrainer :use
HyperTrainer ..> Net : initialize
NetTrainer --> Net : use

Net *-- ActivateFunction : has
Net *-- Grad : has
Net o-- WeightDecay : has
Net --> LayerFactory : use

Net <|.. SimpleNet : 実装

ActivateFunction <|.. Sigmoid
ActivateFunction <|.. ReLu
ActivateFunction <|.. Tanh
ActivateFunction <|.. IdentityMapping : 恒等写像
ActivateFunction <|.. Softmax

LayerFactory <|.. Seq : (動作確認用)
LayerFactory <|.. Random
LayerFactory <|.. Xavier
LayerFactory <|.. He
LayerFactory <|.. Normalize : 標準化スコア化

Grad <|.. Static : 0.001固定
Grad <|.. Shrink : 0.001÷(学習回数+1)
Grad <|.. Moment
Grad <|.. AdGrad
Grad <|.. Adam

WeightDecay <|.. NoDecay
WeightDecay <|.. L1Decay : L1
WeightDecay <|.. L2Decay : L2
WeightDecay <|.. LmaxDecay : L∞


@enduml