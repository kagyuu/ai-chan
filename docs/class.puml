@startuml

class HyperTrainer {
  ハイパーパラメータ を決定するための実験を行う
  ..
  pre [None, Normalize, Singular]
  mid_depth [1,2,3,4,5,6]
  mid_width [5,10,20,40,60,80]
  afunc [Sigmoid, ReLu, Tanh]
  grad [Static1, Static2, Static3, Moment, AdGrad, Adam]

}

class NetTrainer {
  与えられた Net の学習を行う(w,bの調整をする)
  ..
  net
  best_net = None
  min_err = 10e38
  train(loop, x, d) : err[]
}

abstract class Net {
w [重み]
b [バイアス]
f [活性化関数]
g 重み管理
u [forward時の各層の出力の記録]
z [forward時の各層の出力の記録]
drop_rate = 0.0
w_mask [ドロップアウト用マスク]
b_mask [ドロップアウト用マスク]
add_pre_layer(layer_factory, activate_function, x, d)
add_mid_layer([units], layer_factory, activate_function)
add_post_layer(units, layer_factory, activate_function)
drop_out(rate)
drop_in()
forward(x) : y
backward(d,y)
}

interface LayerFactory {
Layer (w,b,f) の初期化を行う
..
create()
}

interface ActivateFunction{
活性化関数
..
calc(x)
differential(x)
delta(d,y)
}

interface Grad {
重み・バイアスの調整
..
dW [重み学習率]
dB [バイアス学習率]
init()
update(dWdE, dWdB)
}

HyperTrainer --> NetTrainer :use
HyperTrainer ..> Net : initialize
NetTrainer --> Net : use

Net *-- ActivateFunction : has
Net *-- Grad : has
Net --> LayerFactory : use

Net <|.. CPUNet : 実装
Net <|.. GPUNet : 実装 (将来構想)

ActivateFunction <|.. Sigmoid : 実装
ActivateFunction <|.. ReLu : 実装
ActivateFunction <|.. Tanh : 実装
ActivateFunction <|.. IdentityMapping : 実装

LayerFactory <|.. Seq : 実装(動作確認用)
LayerFactory <|.. Random : 実装
LayerFactory <|.. Xavier : 実装
LayerFactory <|.. He : 実装
LayerFactory <|.. Normalize : 実装

Grad <|.. Static : 実装
Grad <|.. Moment : 実装
Grad <|.. AdGrad : 実装
Grad <|.. Adam : 実装

@enduml